{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MCMC to fit real data\n",
    "\n",
    "### AY 128/256 (UC Berkeley, 2021)\n",
    "\n",
    "* Let's look at a famous dataset\n",
    "<img src=\"hubble_diagram.jpg\" width=\"90%\">\n",
    "* Hubble (1929) finds that more distant galaxies have larger positive velocities than closer galaxies\n",
    "* Ends up being a big deal\n",
    "* What were some of our crisiticms of this plot, and by extension the result, from our first meeing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our challenge\n",
    "* Hubble found $H_0 \\sim 500$ km/s/Mpc, more or less fitting data with no error bars by eye.\n",
    "* Let's see how well this result stands up to more detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Wrangling\n",
    "* We need to get Hubble's data\n",
    "* Fortunately, it's in Table 1 of Hubble (1929)\n",
    "<img src=\"hubble_table.jpeg\" width=\"80%\">\n",
    "\n",
    " - $m_s$ = photographic magnitude of brightest stars involved.\n",
    "\n",
    " - $r$ = distance in units of $10^6$ parsecs. The first two are Shapley’s values.\n",
    "\n",
    " - $v$ = measured velocities in $km./sec$.    N. G. C. 6822, 221, 224 and 5457 are recent determinations by Humason.\n",
    "\n",
    " - $m_t$ = Holetschek’s visual magnitude as corrected by Hopmann. The first three objects were not measured by Holetschek, and the values of mt represent estimates by the author based upon such data as are available.\n",
    "\n",
    " - $M_t$ = total visual absolute magnitude computed from m_t and $r$.\n",
    "\n",
    "* Since it's a short table, I turned this into a csv file by hand.  It contains galaxy name, distance, velocity, and absolute magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Visualization\n",
    "* Let's use Pandas to read in the data and try to re-create Hubble's plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "\n",
    "# read in the csv file; column names are in the file already\n",
    "df = pd.read_csv('hubble1929.tab1')\n",
    "\n",
    "import pylab as plt\n",
    "# plot distance and velocity\n",
    "plt.scatter(df['distance'], df['velocity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's make this plot look a bit nicer using Seaborn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale=1.3)\n",
    "\n",
    "cmap = plt.get_cmap('plasma')\n",
    "\n",
    "plt.scatter(df['distance'], df['velocity'], c=df['absMag'], s=150, edgecolor='0.7', cmap=cmap)\n",
    "plt.xlabel('Distance (Mpc)', size=24)\n",
    "plt.ylabel('Velocity (km/s)', size=24)\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(\"Absolute Magnitude\", rotation=270, labelpad=20, fontsize=18)\n",
    "\n",
    "# make colorbar brighest to faintest\n",
    "cb.ax.invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Write down a model for the data\n",
    "* $y_n = m~x_n + b$\n",
    " - Linear model to descibe velocity as a function of distance\n",
    " - 2 parameters to fit for: $m$ (slope) and $b$ (offset)\n",
    " - $y_n$: velocity of the nth galaxy; $x_n$: distance to the nth galaxy\n",
    " - We have no error bars (i.e., no $\\sigma_n$) -- is this a problem? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Error bars\n",
    "* Let's fit for the scatter in the data (or the size of the typical uncertainty)\n",
    "* Assume errors are normally distributed\n",
    "* error term: $logsig = log(\\sigma)$\n",
    "* why use $log(\\sigma)$ instead of $\\sigma$ as a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside Review Bayes's theorem\n",
    "* Bayes's theorem: $P(\\theta \\mid \\{y_n\\}) \\propto P(\\{y_n\\} \\mid \\theta) \\, P(\\theta)$\n",
    " - $\\theta = \\{m,b,\\log(\\sigma)\\}$\n",
    "* Assume we have $N$ independent data points (i.e., galaxies in this case).  The posterior for the entire dataset is the product of the posteriors for each data point.\n",
    "\n",
    " - $P(\\theta \\mid \\{y_n\\}) = \\prod_n^N P(\\theta \\mid y_n) \\propto\\ \\prod_n^N P( y_n \\mid \\theta)\\, P(\\theta)$ \n",
    " \n",
    "\n",
    "* Or in log-space:\n",
    " - $\\log P(\\theta \\mid \\{y_n\\}) \\propto \\sum_n^N \\log P(y_n \\mid \\theta) + \\log P(\\theta)$\n",
    " - why work in log-space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write down priors and log-likelihood\n",
    "\n",
    "* Uniform prior on $m$:\n",
    "\n",
    " $$\n",
    "P(m)=\n",
    "\\begin{cases}\n",
    "1 \\, \\rm{if} \\, 100 < m < 1000\\\\\n",
    "0 \\, {\\rm otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Uniform prior on $b$:\n",
    "\n",
    " $$\n",
    "P(b)=\n",
    "\\begin{cases}\n",
    "1 \\, \\rm{if} -500 < b < 500\\\\\n",
    "0 \\, {\\rm otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Uniform prior on $\\log(\\sigma)$:\n",
    "\n",
    " $$\n",
    "P(b)=\n",
    "\\begin{cases}\n",
    "1 \\, \\rm{if} -10 < \\log(\\sigma) < 10\\\\\n",
    "0 \\, {\\rm otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    " \n",
    "\n",
    "* Likelihood function for a normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\{y_n\\} \\mid \\{m, b, \\log(\\sigma)\\}) = \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y_n - mx_n - b)^2}{2 \\sigma_n^2}}\n",
    "\\end{equation}\n",
    "\n",
    " - where $x_n$ is shorthand for the 'distance' variable\n",
    "\n",
    "\n",
    "* log-Likelihood function for a normal distribution:\n",
    "\n",
    " $ \\log P(\\{y_n\\} \\mid \\{m, b, \\log(\\sigma)\\} = -\\frac{1}{2} \\sum^{N}_{n=1} \\Big(\\frac{(y_n - mx_n - b)^2}{\\sigma_n^2} + \\log(2 \\pi \\sigma_n^2) \\Big)$\n",
    " \n",
    "   - how might this change if we had values for $\\sigma_n$ already?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of writing this model:\n",
    "* $m \\sim {\\rm Uniform}(100, 1000)$\n",
    "* $b \\sim {\\rm Uniform}(-500, 500)$\n",
    "* $\\log(\\sigma) \\sim {\\rm Uniform}(-10,10)$\n",
    "* $y_n \\sim {\\rm Normal}(m x_n +b, \\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 5 & 6: Implement and run the model in python using PyMC3, make some diagnostic plots\n",
    "<a href=\"https://docs.pymc.io/api/distributions/continuous.html\">This</a> is a reference to the types of distriutions availabe in PyMC3, which may be handy to you.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "# set up the model\n",
    "with pm.Model() as model:\n",
    "    \n",
    "    # define priors\n",
    "    m = pm.Uniform(\"m\", lower=100, upper=1000)\n",
    "    b = pm.Uniform(\"b\", lower=-500, upper=500)\n",
    "    logsig = pm.Uniform(\"logsig\", lower=-10, upper=10)\n",
    "    \n",
    "    # define the log-likelihood function\n",
    "    # note that numpy doesn't play nicely with PyMC, so you should use their built in math functions\n",
    "    pm.Normal(\"obs\", mu = m*df['distance'].values+b, \n",
    "              sigma=pm.math.exp(logsig), observed=df['velocity'].values)\n",
    "\n",
    "    # now set up the model to run\n",
    "    # default of PyMC is to use the no-turn sampler (NUTS)\n",
    "    \n",
    "    # pm.sample will run the sampler and store output in 'trace' \n",
    "    trace = pm.sample(draws=1000, tune=1000, chains=2, cores=2)\n",
    "    \n",
    "    # traceplot is a routine for plotting the 'traces' from the samples\n",
    "    _ = pm.traceplot(trace, var_names=[\"m\", \"b\", \"logsig\"])\n",
    "    \n",
    "    # pm.summary provides some useful summary and convergance statistics\n",
    "    pm.summary(trace, var_names=[\"m\", \"b\", \"logsig\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace, var_names=[\"m\", \"b\", \"logsig\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# corner is a nice routine for visualizing the marginalized posterior distributions\n",
    "# 1d plots (1d histograms) are called marginalized distributions\n",
    "# 2d plots (2d histograms) are called joint (marginazlied) distributions\n",
    "\n",
    "import corner\n",
    "\n",
    "# translate trace into pandas dataframe for plotting (you can also plot numpy arrays with corner)\n",
    "samples = pm.trace_to_dataframe(trace, varnames=[\"m\", \"b\", \"logsig\"])\n",
    "\n",
    "# make the corner plot and plot results from Hubble's paper as 'truth'\n",
    "# overplot percentiles: 16, 50, 84 on 1d historgrams\n",
    "_ = corner.corner(samples, truths=[500, 0, None], quantiles=[.16, .50, .84])\n",
    "\n",
    "# how big is the scatter/typical error in the data?\n",
    "# let's use the median of logsig for this\n",
    "print(f\"The typical error in velocity is ~{np.int(np.median(np.exp(trace['logsig'])))} km/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Posterior Predictive check\n",
    "* the technical term that means to plot your model on top of the data and do a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make this plot look a bit nicer using Seaborn\n",
    "\n",
    "#import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale=1.3)\n",
    "plt.scatter(df['distance'], df['velocity'], c=df['absMag'], s=150, edgecolor='0.7', cmap=cmap)\n",
    "plt.xlabel('Distance (Mpc)', size=24)\n",
    "plt.ylabel('Velocity (km/s)', size=24)\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(\"Absolute Magnitude\", rotation=270, labelpad=20, fontsize=18)\n",
    "\n",
    "# make an array of 'distances'\n",
    "dist = np.arange(0,2.1, 0.1)\n",
    "\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# this code loops over the trace array and pulls 500 random sets of m,b, and logsig\n",
    "# it then generates 500 posterior models (i.e., lines in velocity and distance space) and overplots them on the data\n",
    "\n",
    "for i in np.random.randint(len(trace) * trace.nchains, size=500):\n",
    "    plt.plot(dist, samples['m'][i]*dist + samples['b'][i], color=\"k\", lw=0.1, alpha=0.3, zorder=0)\n",
    "\n",
    "# overplot Hubble's values    \n",
    "plt.plot(dist, 500*dist, color='r')\n",
    "\n",
    "\n",
    "# tried to invert the y-axis, but ticklabels all disappear\n",
    "#cb.ax.invert_yaxis()\n",
    "#cb.set_ticks(np.arange(-13, -19, 2))\n",
    "#cb.set_ticklabels(('-13', '-15', '-17', '-19'))\n",
    "\n",
    "# should also add a legend of what the lines mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Report some results\n",
    "* what numbers might we report in the abstract of a paper?\n",
    "* it's hard to succinctly describe a distribution/posterior PDF, so we often give a point estimate (i.e., a number with error bars)\n",
    "* some options: MAP (maximum a posteriori), highest probablity density, mean, percentiles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with percentiles\n",
    "\n",
    "from IPython.display import display, Math\n",
    "\n",
    "\n",
    "units =[r'{\\, \\rm km/s/Mpc}', r'{\\, \\rm km/s}', r'{\\, \\rm km/s}']\n",
    "labels = ['m', 'b', r'\\sigma']\n",
    "\n",
    "# we'll want logsig in linear units for the final comparison\n",
    "\n",
    "for i, j in enumerate(samples.keys()):\n",
    "    if j == 'logsig':\n",
    "        summary = np.percentile(np.exp(samples[j]), [16, 50, 84])\n",
    "    else:\n",
    "        summary = np.percentile(samples[j], [16, 50, 84])\n",
    "    diff = np.diff(summary)\n",
    "    mathtext = \"\\mathrm{{{3}}} = {0:.1f}_{{-{1:.1f}}}^{{{2:.1f}}}\"\n",
    "    mathtext = mathtext.format(summary[1], diff[0], diff[1], labels[i])\n",
    "    display(Math(mathtext + \"  \"+ units[i]))\n",
    "    \n",
    " # probably too many significant digits   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
