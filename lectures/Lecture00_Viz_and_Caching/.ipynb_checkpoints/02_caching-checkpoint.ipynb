{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ../talktools.py\n",
    "!rm galaxy1000.csv\n",
    "!rm test.csv\n",
    "!rm -r joblib_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (Simplistic) Introduction to Data Caching\n",
    "\n",
    "<div style=\"text-align: center\"><font size=-1>AY128/256 UC Berkeley (2024)</font> </div> \n",
    "\n",
    "<quote>\n",
    "    <i><font color=\"grey\">\"There are two hard things in Computer Science, cache invalidation, naming things, and off-by-one errors.\"</i></font>\n",
    "    <div style=\"text-align: right\"><font color=\"grey\">--Socrates</font></div>\n",
    "\n",
    "</quote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Motivation</b>: In data-intensive workloads, like the ones you'll be building in this class, it's a good idea to think about your data caching strategies up front. During exploratory analysis, especially when interacting with external sources of databases (e.g., flat files hosted remotely, external databases, etc.) sensible caching can save you a bunch of time in not needing to (re)download data. \n",
    "\n",
    "Even for pure local interactions, you might want to cache the results of computation to avoid having to rerun long-running/expensive calcuations.\n",
    "\n",
    "In the context of reproducbility/replicability, having a cache/store of the data inputs will make it easier for others to redo the steps you did and get the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent or Ephemeral?\n",
    "\n",
    "Every time you run an expensive computation or do a time-intensive query, you implicitly store the results in emphemeral Python object.\n",
    "\n",
    "Let's get a CSV file with some data about galaxies from SDSS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import requests  # requests is the standard URL\n",
    "\n",
    "external_file_location = \"https://raw.githubusercontent.com/AstroHackWeek/AstroHackWeek2014/master/day4/galaxy1000.csv\"\n",
    "local_filename = os.path.basename(external_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTEMPT 1: download the CSV and load it into a DataFrame\n",
    "r = requests.get(external_file_location)\n",
    "df = pd.read_csv(io.StringIO(r.text))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great: we can use the pandas DataFrame `df` in the downstream analysis. The **problem of course is that we rerun our notebook, we need to redownload the data**. This ephemeral caching is usually just fine when the data is small and the computations are quick, but generally slows us down with larger files and beefier computations. \n",
    "\n",
    "Why don't we modify the above to check to see if the file exists and load it in if so. That is, let's create a persistent store of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_galaxy_dataframe():\n",
    "    \"\"\"\n",
    "    get the galaxy CSV file from SDSS if we dont have it already\n",
    "    and parse it into a pandas DataFrame. Save the file\n",
    "    for future use.\n",
    "    \n",
    "    Returns: dataframe\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_filename):\n",
    "        start = time.time()\n",
    "        r = requests.get(external_file_location)\n",
    "\n",
    "        # note: below is for smallish files. To download and save larger files\n",
    "        # see https://stackoverflow.com/a/14114741\n",
    "        with open(local_filename, 'w') as handle:\n",
    "            handle.write(r.text)\n",
    "\n",
    "        print(f\"Wrote the file {local_filename} to disk\")\n",
    "        # from the Python object `r` instead of from disk to avoid disk IO\n",
    "        df = pd.read_csv(io.StringIO(r.text))\n",
    "        print(f\"  Total time: {time.time() - start:0.3} sec\")\n",
    "    else:\n",
    "        start = time.time()\n",
    "        print(f\"Reading the file {local_filename} from disk\")\n",
    "        df = pd.read_csv(local_filename)\n",
    "        print(f\"  Total time: {time.time() - start:0.3} sec\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm galaxy1000.csv\n",
    "get_galaxy_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, the capability (download and save, otherwise load) might be something we want to use a lot. Let's make the function a big more generic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_parse_cached_remote_csvfile(url, local_filename=None, verbose=True):\n",
    "    \"\"\"\n",
    "    get a CSV file at `url` if we dont have it already\n",
    "    and parse it into a pandas DataFrame. Save the file\n",
    "    for future use. Guess the output filename if not given.\n",
    "    \n",
    "    Returns: dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # here we might error check to see if the URL is valid, points to a CSV file\n",
    "    # etc.\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "        \n",
    "    if not os.path.exists(local_filename):\n",
    "        start = time.time()\n",
    "        r = requests.get(url)\n",
    "\n",
    "        # note: below is for smallish files. To download and save larger files\n",
    "        # see https://stackoverflow.com/a/14114741\n",
    "        with open(local_filename, 'w') as handle:\n",
    "            handle.write(r.text)\n",
    "        if verbose:\n",
    "            print(f\"Wrote the file {local_filename} to disk\", flush=True)\n",
    "        # from the Python object `r` instead of from disk to avoid disk IO\n",
    "        df = pd.read_csv(io.StringIO(r.text))\n",
    "        if verbose:\n",
    "            print(f\"  Total time: {time.time() - start:0.3} sec\")\n",
    "    else:\n",
    "        start = time.time()\n",
    "        if verbose:\n",
    "            print(f\"Reading the file {local_filename} from disk\")\n",
    "        df = pd.read_csv(local_filename)\n",
    "        if verbose:\n",
    "            print(f\"  Total time: {time.time() - start:0.3} sec\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_and_parse_cached_remote_csvfile(external_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_and_parse_cached_remote_csvfile(external_file_location, local_filename=\"test.csv\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a similar workflow for external queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created TAP+ (v1.2.1) - Connection:\n",
      "\tHost: gea.esac.esa.int\n",
      "\tUse HTTPS: True\n",
      "\tPort: 443\n",
      "\tSSL Port: 443\n",
      "Created TAP+ (v1.2.1) - Connection:\n",
      "\tHost: geadata.esac.esa.int\n",
      "\tUse HTTPS: True\n",
      "\tPort: 443\n",
      "\tSSL Port: 443\n"
     ]
    }
   ],
   "source": [
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astroquery.gaia import Gaia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\" -- Get some nearby sources\n",
    "        SELECT top 1000 \n",
    "            ra,dec,source_id,parallax,pm\n",
    "        FROM gaiaedr3.gaia_source \n",
    "        WHERE parallax_over_error > 15.0\n",
    "            AND parallax > 200.0\n",
    "        ORDER BY parallax DESC\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.71 sec\n",
      "        ra                 dec              source_id           parallax          pm   \n",
      "       deg                 deg                                    mas          mas / yr\n",
      "------------------ ------------------- ------------------- ------------------ ---------\n",
      "217.39232147200883  -62.67607511676666 5853498713190525696  768.0665391873573  3859.228\n",
      "269.44850252543836   4.739420051112412 4472832130942575872   546.975939730948 10393.349\n",
      "164.10319030755974   7.002726940984864 3864972938605115520 415.17941567802137 4715.3296\n",
      "165.83095967577933  35.948653032660104  762815470562110464 392.75294543876464   4811.68\n",
      "101.28662552099249 -16.720932526023173 2947050466531873024 374.48958852876103  1024.399\n",
      "24.771674208211856 -17.947682860008488 5140693571158946048  373.8443122683992 3231.9072\n",
      "24.771554293454546 -17.948299887129313 5140693571158739840 367.71189618147696 3428.8074\n",
      " 282.4587890175222  -23.83709744872712 4075141768785646848  336.0266016683708  668.1399\n",
      " 355.4800152581559  44.170375700747755 1926461164913660160 316.48118678226916 1595.6226\n",
      " 53.22829341517546  -9.458168216292322 5164707970261890560  310.5772928005821  974.9817\n",
      "               ...                 ...                 ...                ...       ...\n",
      "176.45664661730092    -64.843052846102 5332606522595645952 215.67527344818546 2683.8972\n",
      "298.48192590176114   44.41290615922913 2079073928612821760  214.5745030256979  593.9394\n",
      " 343.3241110018443 -14.266689393516607 2603090003484152064  214.0379888743159 1170.8784\n",
      "161.08527551332452  -61.20263830459439 5254061535106490112 206.96977507456424 1647.8783\n",
      "1.6763504259443918 -7.5464753252597605 2441630500517079808 206.35003920573322 2059.8635\n",
      " 43.77198593477078  -47.01672835622661 4752399493622045696 205.42508538452748 1154.1206\n",
      " 152.8329289582321  49.451988897100506  823773494718931968 205.31478196510724 1454.0824\n",
      "166.34205974427476   43.53094856108456  778947814402602752  203.8875927696954  4505.308\n",
      "166.35208967877313   43.52590762036144  778947608243864320 203.83234385738783 4444.9106\n",
      "154.89881370110675  19.869809905451323  625453654702751872 201.40642101948626 500.50797\n",
      " 323.3912518776402  -49.01263039681064 6562924609150908416  201.3251983325545  818.1644\n",
      "Length = 50 rows\n"
     ]
    }
   ],
   "source": [
    "def get_gaia_query(q):\n",
    "    start = time.time()\n",
    "    job = Gaia.launch_job(q)\n",
    "    print(f\"Total time: {time.time()-start:0.2f} sec\")\n",
    "    return job.get_results()\n",
    "\n",
    "result_table = get_gaia_query(query)\n",
    "print(result_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great: we can use `result_table` in the downstream analysis. The problem of course is that we rerun our notebook we have to download the data again (and the computation on the remote side needs to happen as well. This isn't ecofriendly!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be better off memorizing the result for a given input and save the data locally. We could do this \"by hand\" but [`joblib`](https://joblib.readthedocs.io/en/latest/) was built for just this (and parallel computing):\n",
    "\n",
    "<pre>\n",
    "    The Memory class defines a context for lazy evaluation of function, by putting the results in a store, by default using a disk, and not re-running the function twice for the same arguments. It works by explicitly saving the output to a file and it is designed to work with non-hashable and potentially large input and output data types such as numpy arrays.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "cachedir = './joblib_cache'\n",
    "memory = Memory(cachedir, verbose=0, bytes_limit=1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def get_gaia_query(q):\n",
    "    start = time.time()\n",
    "    job = Gaia.launch_job(q)\n",
    "    print(f\"Total time: {time.time()-start:0.2f} sec\")\n",
    "    return job.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `memory` as a function decorator. Every time `get_gaia_query` is called now, the variable `q` will be checked. If we've seen this before, then joblib will just return the output from last time. Otherwise we'll run this function and save the results in the `joblib_cache` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.49 sec\n"
     ]
    }
   ],
   "source": [
    "result_table = get_gaia_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.89 ms, sys: 1.62 ms, total: 6.51 ms\n",
      "Wall time: 5.24 ms\n"
     ]
    }
   ],
   "source": [
    "%time result_table = get_gaia_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+1 color=red>**Question**</font>: In what case's might you NOT want to cache query results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_alerts(area=\"CA\"):\n",
    "    r = requests.get(f\"https://api.weather.gov/alerts/active?area={area}\")\n",
    "    zone = r.json()[\"features\"][0][\"properties\"][\"areaDesc\"]\n",
    "    return f'{zone}: {r.json()[\"features\"][0][\"properties\"][\"headline\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Northern Trinity: Winter Storm Warning issued January 11 at 3:04PM PST until January 13 at 9:00PM PST by NWS Eureka CA\n"
     ]
    }
   ],
   "source": [
    "print(get_weather_alerts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
